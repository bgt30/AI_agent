import os
import streamlit as st
from dotenv import load_dotenv
from googleapiclient.discovery import build
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import re
from transformers import pipeline
from konlpy.tag import Okt

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")

# GPT-4o-mini ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(
    temperature=0,  # ëª¨ë¸ì˜ ì‘ë‹µ ì˜¨ë„ ì„¤ì •
    model_name="gpt-4o-mini",  # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
    openai_api_key=openai_api_key  # OpenAI API í‚¤
)

# YouTube API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
youtube = build("youtube", "v3", developerKey=YOUTUBE_API_KEY)

# ë¶„ë¥˜ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (AI ê´€ë ¨ ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ê¸° ìœ„í•œ í…œí”Œë¦¿)
classify_prompt = PromptTemplate(
    input_variables=["query"],
    template="""
    ë‹¹ì‹ ì€ ì¸ê³µì§€ëŠ¥(AI)ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì— ë‹µë³€ì„ ì œê³µí•˜ê³  ìœ íŠœë¸Œ ë™ì˜ìƒì„ ì¶”ì²œí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

    ê´€ë ¨ì„± íŒë‹¨:
       ì‚¬ìš©ìì˜ ì§ˆë¬¸: "{query}"
       - ì§ˆë¬¸ì´ AI, ì¸ê³µì§€ëŠ¥ ë˜ëŠ” ê·¸ ì‘ìš© ë¶„ì•¼ì™€ ì§ì ‘ì  í˜¹ì€ ê°„ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.
       - ê´€ë ¨ì´ ìˆë‹¤ë©´ "AI-RELATED"ë¡œ ì‹œì‘í•˜ì„¸ìš”.
       - ê´€ë ¨ì´ ì—†ë‹¤ë©´ "NOT-AI-RELATED"ë¡œ ì‹œì‘í•˜ê³ , ë” ì´ìƒ ì§„í–‰í•˜ì§€ ë§ˆì„¸ìš”.
    """
)
# LLMChainì„ ì´ìš©í•´ AI ê´€ë ¨ì„± íŒë‹¨ ëª¨ë¸ ìƒì„±
classify_chain = LLMChain(llm=llm, prompt=classify_prompt)

# Hugging Face ëª¨ë¸ ì´ˆê¸°í™”
nlp_model = pipeline("feature-extraction", model="sentence-transformers/all-MiniLM-L6-v2")

# í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
okt = Okt()

# ì§ˆë¬¸ ë¶„ì„ ë° ì¿¼ë¦¬ ì •ì œ í•¨ìˆ˜
def refine_query_with_nlp_and_tokenizer(query):
    # 1. NLPë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
    embeddings = nlp_model(query)
    keywords = " ".join([re.sub(r'[^\w\s]', '', word) for word in query.split()])

    # 2. í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ ì¡°ì‚¬/ì ‘ë¯¸ì‚¬ ì œê±°
    tokens = okt.nouns(query)
    refined_tokens = " ".join(tokens)

    # NLP ê¸°ë°˜ í‚¤ì›Œë“œì™€ í˜•íƒœì†Œ ë¶„ì„ í‚¤ì›Œë“œë¥¼ ê²°í•©
    return f"{keywords} {refined_tokens}".strip()

# ìœ íŠœë¸Œ ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_top_comments(video_id, max_comments=5):
    try:
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=max_comments,
            order="relevance"
        )
        response = request.execute()
        comments = [
            item["snippet"]["topLevelComment"]["snippet"]["textOriginal"]
            for item in response["items"]
        ]
        return comments
    except Exception:
        return ["ëŒ“ê¸€ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]

# ìœ íŠœë¸Œ ì˜ìƒì„ ê²€ìƒ‰í•˜ê³  ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def search_youtube_videos_with_comments(query, max_results=5, max_comments=3):
    refined_query = refine_query_with_nlp_and_tokenizer(query)
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform([refined_query])

    request = youtube.search().list(
        part="snippet",
        q=refined_query,
        type="video",
        maxResults=max_results,
        order="viewCount"
    )
    response = request.execute()
    videos = []
    if "items" in response:
        for item in response["items"]:
            title = item["snippet"]["title"]
            description = item["snippet"]["description"]
            video_id = item["id"]["videoId"]

            video_text = title + " " + description
            video_text = refine_query_with_nlp_and_tokenizer(video_text)
            video_tfidf_matrix = tfidf_vectorizer.transform([video_text])

            similarity = cosine_similarity(tfidf_matrix, video_tfidf_matrix)[0][0]
            comments = get_top_comments(video_id, max_comments)

            videos.append({
                "title": title,
                "description": description,
                "video_id": video_id,
                "similarity": similarity,
                "comments": comments
            })

    videos = sorted(videos, key=lambda x: x["similarity"], reverse=True)
    return videos

# Streamlit UI ì„¤ì •
st.title("ìœ íŠœë¸Œ ì˜ìƒ ì¶”ì²œ AIğŸ¤–")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”! ì¢…ë£Œí•˜ë ¤ë©´ 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”."):
    if user_input.lower() == "exit":
        st.info("ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.chat_message("assistant"):
            try:
                classification = classify_chain.run({"query": user_input})
                if classification.startswith("NOT-AI-RELATED"):
                    error_message = "AI ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”"
                    st.subheader(error_message)
                    st.session_state.messages.append({"role": "assistant", "content": error_message})
                else:
                    videos = search_youtube_videos_with_comments(user_input)
                    if videos:
                        st.subheader("ê´€ë ¨ ìœ íŠœë¸Œ ì˜ìƒ ì¶”ì²œ:")
                        assistant_response = ""
                        for video in videos:
                            st.video(f"https://www.youtube.com/watch?v={video['video_id']}")
                            video_info = f"**ì„¤ëª…:** {video['description']}\n"
                            comments_info = "\n".join([f"  - {comment}" for comment in video["comments"]])
                            full_info = f"{video_info}\n\n**ëŒ“ê¸€:**\n{comments_info}\n\n---"
                            assistant_response += full_info
                            st.markdown(full_info)
                        st.session_state.messages.append({"role": "assistant", "content": assistant_response})
                    else:
                        error_message = "ê´€ë ¨ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        st.subheader(error_message)
                        st.session_state.messages.append({"role": "assistant", "content": error_message})
            except Exception as e:
                st.error(f"RAG í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
